{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45fa9a45",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Install/Verify Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14c0bce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-genai>=1.0.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (1.18.0)\n",
      "Requirement already satisfied: pinecone>=3.0.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (6.0.2)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (1.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from google-genai>=1.0.0) (4.9.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from google-genai>=1.0.0) (2.40.2)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from google-genai>=1.0.0) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from google-genai>=1.0.0) (2.11.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from google-genai>=1.0.0) (2.32.3)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from google-genai>=1.0.0) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from google-genai>=1.0.0) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from pinecone>=3.0.0) (2025.1.31)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from pinecone>=3.0.0) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from pinecone>=3.0.0) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from pinecone>=3.0.0) (2.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from anyio<5.0.0,>=4.8.0->google-genai>=1.0.0) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from anyio<5.0.0,>=4.8.0->google-genai>=1.0.0) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from anyio<5.0.0,>=4.8.0->google-genai>=1.0.0) (1.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai>=1.0.0) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai>=1.0.0) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai>=1.0.0) (4.9.1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from httpx<1.0.0,>=0.28.1->google-genai>=1.0.0) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai>=1.0.0) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.0.0->google-genai>=1.0.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.0.0->google-genai>=1.0.0) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.0.0->google-genai>=1.0.0) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from python-dateutil>=2.5.3->pinecone>=3.0.0) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from requests<3.0.0,>=2.28.1->google-genai>=1.0.0) (3.3.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai>=1.0.0) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Required libraries checked/installed.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"google-genai>=1.0.0\" \"pinecone>=3.0.0\" python-dotenv tqdm\n",
    "\n",
    "print(\"Required libraries checked/installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c67fd7",
   "metadata": {},
   "source": [
    "## Load Environment Variables & Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df69b55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment variables...\n",
      ".env file loaded.\n",
      "Initializing Google Gen AI Client...\n",
      "Google Gen AI Client initialized successfully.\n",
      "Initializing Pinecone connection...\n",
      "Pinecone client initialized successfully.\n",
      "Connected to Pinecone index 'semantic-search-app-index'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from google import genai\n",
    "from pinecone import Pinecone\n",
    "\n",
    "\n",
    "# Load API Keys\n",
    "print(\"Loading environment variables...\")\n",
    "dotenv_path = find_dotenv(raise_error_if_not_found=False)\n",
    "if dotenv_path:\n",
    "    load_dotenv(dotenv_path)\n",
    "    print(\".env file loaded.\")\n",
    "else:\n",
    "    print(\"Warning: .env file not found. Will attempt to load API keys from system environment variables if they are set. \"\n",
    "          \"Otherwise, please create a .env file with your API keys.\")\n",
    "\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "if not google_api_key:\n",
    "    raise EnvironmentError(\"GOOGLE_API_KEY not found. Please set it in .env or as a system environment variable.\")\n",
    "if not pinecone_api_key:\n",
    "    raise EnvironmentError(\"PINECONE_API_KEY not found. Please set it in .env or as a system environment variable.\")\n",
    "\n",
    "# Initialize Google Gen AI Client (for embedding and generation)\n",
    "print(\"Initializing Google Gen AI Client...\")\n",
    "google_ai_client = None\n",
    "google_ai_ready = False\n",
    "\n",
    "try:\n",
    "    google_ai_client = genai.Client(api_key=google_api_key)\n",
    "    print(\"Google Gen AI Client initialized successfully.\")\n",
    "    google_ai_ready = True\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Google Gen AI Client: {e}\")\n",
    "\n",
    "# Initialize Pinecone Connection\n",
    "print(\"Initializing Pinecone connection...\")\n",
    "pc = None\n",
    "\n",
    "try:\n",
    "    pc = Pinecone(api_key=pinecone_api_key)\n",
    "    print(\"Pinecone client initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Pinecone client: {e}\")\n",
    "\n",
    "# Connect to the Pinecone Index\n",
    "pinecone_index = None\n",
    "index_name = 'semantic-search-app-index'  # Same index name used in Step 3 notebook (02-embedding-storage.ipynb)\n",
    "\n",
    "if pc:\n",
    "    try:\n",
    "        if index_name in [index_info[\"name\"] for index_info in pc.list_indexes()]:\n",
    "            pinecone_index = pc.Index(index_name)\n",
    "            print(f\"Connected to Pinecone index '{index_name}'.\")\n",
    "            # Optional: Describe index stats to confirm it has vectors\n",
    "            # print(pinecone_index.describe_index_stats())\n",
    "        else:\n",
    "            print(f\"Error: Pinecone index '{index_name}' does not exist. Please run the Step 3 notebook (02-embedding-storage.ipynb) to create and populate it.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Pinecone index: {e}\")\n",
    "else:\n",
    "    print(\"Skipping Pinecone index connection due to client initialization failure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ec836f",
   "metadata": {},
   "source": [
    "## Define Embedding Model and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54f9ee80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using embedding model for queries: models/text-embedding-004\n",
      "Target LLM for generation: gemini-2.0-flash\n",
      "Chat session created with gemini-2.0-flash.\n"
     ]
    }
   ],
   "source": [
    "embedding_model_name = None\n",
    "llm_model_identifier = None  # For the string name\n",
    "llm_chat_session = None  # For the chat object\n",
    "\n",
    "if google_ai_ready and google_ai_client:\n",
    "    embedding_model_name = 'models/text-embedding-004'  # For querying\n",
    "    print(f\"Using embedding model for queries: {embedding_model_name}\")\n",
    "\n",
    "    # Generative Model (LLM) - Gemini 2.0 Flash\n",
    "    llm_model_identifier = 'gemini-2.0-flash'\n",
    "    print(f\"Target LLM for generation: {llm_model_identifier}\")\n",
    "    \n",
    "    # Initialize a chat session for multi-turn conversation\n",
    "    try:\n",
    "        # Create a new chat session using the client\n",
    "        llm_chat_session = google_ai_client.chats.create(model=llm_model_identifier)\n",
    "        print(f\"Chat session created with {llm_model_identifier}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating chat session with {llm_model_identifier}: {e}\")\n",
    "        llm_chat_session = None\n",
    "        \n",
    "else:\n",
    "    print(\"Skipping model definitions due to Google AI client configuration error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbbf63e",
   "metadata": {},
   "source": [
    "## RAG Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e640ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai import types\n",
    "\n",
    "def answer_query_with_rag(user_query: str, chat_session: genai.chats.Chat, top_k: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves relevant context from Pinecone and uses Gemini to answer a query\n",
    "    within a given chat session.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's natural language query.\n",
    "        chat_session: The Gemini chat session object.\n",
    "        top_k (int): Number of top results to retrieve from Pinecone (default is 3).\n",
    "\n",
    "    Returns:\n",
    "        str: The model-generated answer or an error message.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if all required components are initialized\n",
    "    if not google_ai_ready or not google_ai_client or not pinecone_index or not chat_session:\n",
    "        return \"Error: Critical components (Google AI Client, Pinecone Index, or Chat Session) not initialized.\"\n",
    "\n",
    "    print(f\"\\nProcessing query: '{user_query}'\")\n",
    "\n",
    "    # 1. Retrieve (Embed query & Search Vector DB)\n",
    "    print(\"Embedding user query...\")\n",
    "    try:\n",
    "        # Use the client object's models attribute for embedding\n",
    "        embedding_response = google_ai_client.models.embed_content(\n",
    "            model=embedding_model_name,\n",
    "            contents=user_query, # Corrected from 'content' to 'contents' for single query\n",
    "            config=types.EmbedContentConfig(task_type=\"RETRIEVAL_QUERY\")\n",
    "        )\n",
    "        query_embedding = embedding_response.embeddings[0].values  # list of floats\n",
    "    except Exception as e:\n",
    "        return f\"Error embedding query: {e}\"\n",
    "\n",
    "    print(\"Searching Pinecone index for relevant chunks...\")\n",
    "    try:\n",
    "        query_results = pinecone_index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=top_k,\n",
    "            include_metadata=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return f\"Error querying Pinecone: {e}\"\n",
    "\n",
    "    # 2. Augment (Create prompt with context)\n",
    "    retrieved_chunks_text = []\n",
    "    if query_results.matches:\n",
    "        for match in query_results.matches:\n",
    "            if match.metadata and 'text' in match.metadata:\n",
    "                retrieved_chunks_text.append(match.metadata['text'])\n",
    "        print(f\"Retrieved {len(retrieved_chunks_text)} chunks.\")\n",
    "    else:\n",
    "        print(\"No relevant chunks found in Pinecone.\")\n",
    "    \n",
    "    context_string = \"\\n\\n---\\n\\n\".join(retrieved_chunks_text)\n",
    "\n",
    "    # Basic Prompt Engineering\n",
    "    # The chat history is managed by the SDK.\n",
    "    # For this turn, only the current query and retrieved context are provided.\n",
    "    prompt_for_current_turn = f\"\"\"\n",
    "    Based ONLY on the following context, answer the question.\n",
    "    If the answer is not found in the context, state \"I cannot answer this question based on the provided information.\"\n",
    "\n",
    "    Context:\n",
    "    {context_string}\n",
    "\n",
    "    Question: {user_query}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Constructed prompt for current turn.\")\n",
    "\n",
    "    # 3. Generate (Call LLM API using the chat session)\n",
    "    print(f\"Sending message to chat session with LLM: {llm_model_identifier}...\")\n",
    "    try:\n",
    "        # Send augmented prompt to chat session\n",
    "        chat_response = chat_session.send_message(prompt_for_current_turn)\n",
    "        final_answer = chat_response.text\n",
    "    except Exception as e:\n",
    "        return f\"Error generating answer with LLM via chat session: {e}\"\n",
    "\n",
    "    print(\"Answer generated.\")\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f401b937",
   "metadata": {},
   "source": [
    "## Test the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4ac5521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test 1: Initial Query ---\n",
      "\n",
      "Processing query: 'What is the core idea of attention mechanism in the Transformer model?'\n",
      "Embedding user query...\n",
      "Searching Pinecone index for relevant chunks...\n",
      "Retrieved 3 chunks.\n",
      "Constructed prompt for current turn.\n",
      "Sending message to chat session with LLM: gemini-2.0-flash...\n",
      "Answer generated.\n",
      "\n",
      "Query: What is the core idea of attention mechanism in the Transformer model?\n",
      "Answer: The Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.\n",
      "\n",
      "\n",
      "--- Test 2: Follow-up Query (utilizing chat history) ---\n",
      "\n",
      "Processing query: 'Can you elaborate on the 'Scaled Dot-Product Attention' part mentioned?'\n",
      "Embedding user query...\n",
      "Searching Pinecone index for relevant chunks...\n",
      "Retrieved 3 chunks.\n",
      "Constructed prompt for current turn.\n",
      "Sending message to chat session with LLM: gemini-2.0-flash...\n",
      "Answer generated.\n",
      "\n",
      "Query: Can you elaborate on the 'Scaled Dot-Product Attention' part mentioned?\n",
      "Answer: The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:\n",
      "Attention(Q, K, V) = softmax(QKT/√dk)V\n",
      "\n",
      "\n",
      "--- Test 3: Query Potentially Not in Document ---\n",
      "\n",
      "New chat session created for Test 3 with gemini-2.0-flash.\n",
      "\n",
      "Processing query: 'What is the capital of France?'\n",
      "Embedding user query...\n",
      "Searching Pinecone index for relevant chunks...\n",
      "Retrieved 1 chunks.\n",
      "Constructed prompt for current turn.\n",
      "Sending message to chat session with LLM: gemini-2.0-flash...\n",
      "Answer generated.\n",
      "\n",
      "Query: What is the capital of France?\n",
      "Answer: I cannot answer this question based on the provided information.\n",
      "\n",
      "\n",
      "--- Chat History (Session 1) ---\n",
      "Role: user - Text: \n",
      "    Based ONLY on the following context, answer the question.\n",
      "    If the answer is not found in the context, state \"I cannot answer this question based on the provided information.\"\n",
      "\n",
      "    Context:\n",
      "    is similar to that of single-head attention with full dimensionality.\n",
      "3.2.3 Applications of Attention in our Model\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      "and the memory keys and values come from the output of the encoder. This allows every\n",
      "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[38, 2, 9].\n",
      "• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
      "and queries come from the same place, in this case, the output of the previous layer in the\n",
      "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
      "encoder.\n",
      "• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
      "\n",
      "---\n",
      "\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "\n",
      "---\n",
      "\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\n",
      "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "\n",
      "    Question: What is the core idea of attention mechanism in the Transformer model?\n",
      "\n",
      "    Answer:\n",
      "    \n",
      "Role: model - Text: The Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.\n",
      "\n",
      "Role: user - Text: \n",
      "    Based ONLY on the following context, answer the question.\n",
      "    If the answer is not found in the context, state \"I cannot answer this question based on the provided information.\"\n",
      "\n",
      "    Context:\n",
      "    where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "3\n",
      "Scaled Dot-Product Attention\n",
      " Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "query with the corresponding key.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\n",
      "values.\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
      "into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\n",
      "the matrix of outputs as:\n",
      "\n",
      "---\n",
      "\n",
      "dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\n",
      "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
      "extremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\n",
      ".\n",
      "3.2.2 Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
      "we found it beneficial to linearly project the queries, keys and values h times with different, learned\n",
      "linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\n",
      "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
      "4To illustrate why the dot products get large, assume that the components of q and k are independent random\n",
      "variables with mean 0 and variance 1. Then their dot product, q · k = Pdk\n",
      "i=1 qiki, has mean 0 and variance dk.\n",
      "4\n",
      "\n",
      "---\n",
      "\n",
      "into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\n",
      "the matrix of outputs as:\n",
      "Attention(Q, K, V) = softmax(QKT\n",
      "√dk\n",
      ")V (1)\n",
      "The two most commonly used attention functions are additive attention [2], and dot-product (multi-\n",
      "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
      "of 1√dk\n",
      ". Additive attention computes the compatibility function using a feed-forward network with\n",
      "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
      "much faster and more space-efficient in practice, since it can be implemented using highly optimized\n",
      "matrix multiplication code.\n",
      "While for small values of dk the two mechanisms perform similarly, additive attention outperforms\n",
      "dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\n",
      "\n",
      "    Question: Can you elaborate on the 'Scaled Dot-Product Attention' part mentioned?\n",
      "\n",
      "    Answer:\n",
      "    \n",
      "Role: model - Text: The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:\n",
      "Attention(Q, K, V) = softmax(QKT/√dk)V\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if google_ai_ready and pinecone_index and llm_chat_session:\n",
    "    # Test query 1 (initial query)\n",
    "    print(\"\\n--- Test 1: Initial Query ---\")\n",
    "    test_query_1 = \"What is the core idea of attention mechanism in the Transformer model?\"\n",
    "    answer_1 = answer_query_with_rag(test_query_1, llm_chat_session)\n",
    "    print(f\"\\nQuery: {test_query_1}\")\n",
    "    print(f\"Answer: {answer_1}\")\n",
    "\n",
    "    # Test query 2 (follow-up)\n",
    "    print(\"\\n--- Test 2: Follow-up Query (utilizing chat history) ---\")\n",
    "    test_query_2 = \"Can you elaborate on the 'Scaled Dot-Product Attention' part mentioned?\"\n",
    "    answer_2 = answer_query_with_rag(test_query_2, llm_chat_session) \n",
    "    print(f\"\\nQuery: {test_query_2}\")\n",
    "    print(f\"Answer: {answer_2}\")\n",
    "\n",
    "    # Test query 3 (new chat session)\n",
    "    print(\"\\n--- Test 3: Query Potentially Not in Document ---\")\n",
    "    try:\n",
    "        new_chat_session_for_test3 = google_ai_client.chats.create(model=llm_model_identifier)\n",
    "        print(f\"\\nNew chat session created for Test 3 with {llm_model_identifier}.\")\n",
    "        test_query_3 = \"What is the capital of France?\"\n",
    "        answer_3 = answer_query_with_rag(test_query_3, new_chat_session_for_test3, top_k=1)\n",
    "        print(f\"\\nQuery: {test_query_3}\")\n",
    "        print(f\"Answer: {answer_3}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Test 3 (new chat session or query): {e}\")\n",
    "        \n",
    "    # Display chat history from the first session\n",
    "    print(\"\\n--- Chat History (Session 1) ---\")\n",
    "    try:\n",
    "        for message in llm_chat_session.get_history():\n",
    "            role = getattr(message, 'role', 'unknown').lower()\n",
    "            text_content = \"\"\n",
    "            if hasattr(message, 'parts') and message.parts:\n",
    "                text_content = getattr(message.parts[0], 'text', '')\n",
    "            print(f\"Role: {role} - Text: {text_content}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving chat history: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping RAG pipeline test due to initialization errors of critical components (Google AI Client, Pinecone Index, or Chat Session).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic_search_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
