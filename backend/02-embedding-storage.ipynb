{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a70c75d",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d0049ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (4.1.0)\n",
      "Requirement already satisfied: pinecone>=3.0.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (6.0.2)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: pypdf in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (5.4.0)\n",
      "Requirement already satisfied: langchain in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (0.3.23)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from pinecone>=3.0.0) (2025.1.31)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from pinecone>=3.0.0) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from pinecone>=3.0.0) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from pinecone>=3.0.0) (2.3.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langchain) (0.3.51)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langchain) (0.3.27)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langchain) (2.11.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from python-dateutil>=2.5.3->pinecone>=3.0.0) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Required libraries checked/installed.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers \"pinecone>=3.0.0\" torch tqdm pypdf langchain # Ensure pinecone-client is recent\n",
    "\n",
    "print(\"Required libraries checked/installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a281f015",
   "metadata": {},
   "source": [
    "## Load Environment Variables (Pinecone Credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b807951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading .env file...\n",
      "Pinecone API Key loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Attempt to find and load the .env file to get environment variables\n",
    "dotenv_path = find_dotenv(raise_error_if_not_found=False) # Avoid error if .env is missing\n",
    "\n",
    "if dotenv_path:\n",
    "    print(\"Loading .env file...\")\n",
    "    load_dotenv(dotenv_path)\n",
    "    pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "    if not pinecone_api_key:\n",
    "        raise ValueError(\"Error: PINECONE_API_KEY not found in .env file.\")\n",
    "    else:\n",
    "        print(\"Pinecone API Key loaded.\")\n",
    "        \n",
    "else:\n",
    "    raise FileNotFoundError(\"Error: .env file not found. Please create one with your Pinecone API key.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0136b3",
   "metadata": {},
   "source": [
    "## Load and Chunk Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b7f0628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF from: ../data/Attention is All You Need.pdf\n",
      "Successfully loaded document with 39602 characters.\n",
      "Initializing text splitter...\n",
      "Splitting document into chunks...\n",
      "Document split into 50 chunks.\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "pdf_file_path = os.path.join('..', 'data', 'Attention is All You Need.pdf')\n",
    "text_chunks = [] # Initialize an empty list for chunks\n",
    "\n",
    "if not os.path.exists(pdf_file_path):\n",
    "    raise FileNotFoundError(f\"Error: PDF file not found at calculated path: {os.path.abspath(pdf_file_path)}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Loading PDF from: {pdf_file_path}\")\n",
    "    try:\n",
    "        # Initialize the PDF reader\n",
    "        reader = PdfReader(pdf_file_path)\n",
    "\n",
    "        # Extract text from the PDF\n",
    "        full_document_text = \"\"\n",
    "        for page_num, page in enumerate(reader.pages):\n",
    "            page_text = page.extract_text()\n",
    "\n",
    "            # Check if text was extracted from current page\n",
    "            if page_text:\n",
    "                full_document_text += page_text + \"\\n\"\n",
    "            else:\n",
    "                print(f\"Warning: No text extracted from page {page_num + 1}.\")\n",
    "\n",
    "        # Check if any text was extracted\n",
    "        if not full_document_text:\n",
    "             print(\"Warning: No text could be extracted from the PDF. Cannot proceed.\")\n",
    "        else:\n",
    "            print(f\"Successfully loaded document with {len(full_document_text)} characters.\")\n",
    "\n",
    "            # Initialize Recursive Character Text Splitter\n",
    "            print(\"Initializing text splitter...\")\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1000,\n",
    "                chunk_overlap=200,\n",
    "                length_function=len,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Priorize splitting by paragraph, then line, then space\n",
    "            )\n",
    "\n",
    "            # Split text into chunks\n",
    "            print(\"Splitting document into chunks...\")\n",
    "            text_chunks = text_splitter.split_text(full_document_text)\n",
    "            print(f\"Document split into {len(text_chunks)} chunks.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during PDF loading or chunking: {e}\")\n",
    "        raise e # Re-raise the exception to stop execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda7267a",
   "metadata": {},
   "source": [
    "## Initialize Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adaed39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Embedding model loaded successfully.\n",
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Check if chunks were created before loading the model\n",
    "if text_chunks:\n",
    "    print(\"Loading embedding model: all-MiniLM-L6-v2\")\n",
    "    try:\n",
    "        # Instantiate the Sentence Transformer model\n",
    "        # Specify device='mps' to try using Apple Silicon GPU if available and PyTorch supports it well\n",
    "        # Or leave as None to let the library decide (often CPU)\n",
    "        embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device=None)\n",
    "        print(\"Embedding model loaded successfully.\")\n",
    "\n",
    "        # Print model details\n",
    "        print(embedding_model)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Sentence Transformer model: {e}\")\n",
    "        embedding_model = None # Ensure it's None if loading fails\n",
    "        \n",
    "else:\n",
    "    print(\"Skipping embedding model initialization as no text chunks were loaded.\")\n",
    "    embedding_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65b78c5",
   "metadata": {},
   "source": [
    "## Initialize Pinecone Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cba2adf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Pinecone connection...\n",
      "Pinecone client initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Check if API key and model were loaded\n",
    "if pinecone_api_key and embedding_model:\n",
    "    print(f\"Initializing Pinecone connection...\")\n",
    "    try:\n",
    "        # Initialize the Pinecone client with only the API key\n",
    "        pc = Pinecone(api_key=pinecone_api_key)\n",
    "        print(\"Pinecone client initialized successfully.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Pinecone client: {e}\")\n",
    "        pc = None # Ensure client object is None on error\n",
    "        \n",
    "else:\n",
    "    print(\"Skipping Pinecone initialization due to missing API key or embedding model.\")\n",
    "    pc = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a71bb1",
   "metadata": {},
   "source": [
    "## Create or Connect to Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fa6f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if Pinecone index 'semantic-search-app-index' exists...\n",
      "Index 'semantic-search-app-index' does not exist. Creating...\n",
      "Embedding dimension: 384\n",
      "Index 'semantic-search-app-index' created successfully. Please wait a moment for it to initialize...\n",
      "Connected to index 'semantic-search-app-index'.\n",
      "{'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {},\n",
      " 'total_vector_count': 0,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "# Check if Pinecone client initialized successfully\n",
    "if pc:\n",
    "    index_name = 'semantic-search-app-index'\n",
    "\n",
    "    # Check existing indexes\n",
    "    print(f\"Checking if Pinecone index '{index_name}' exists...\")\n",
    "    existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "    if index_name not in existing_indexes:\n",
    "        print(f\"Index '{index_name}' does not exist. Creating...\")\n",
    "\n",
    "        # Get the dimensionality of the embedding model\n",
    "        embedding_dim = embedding_model.get_sentence_embedding_dimension()\n",
    "        print(f\"Embedding dimension: {embedding_dim}\")\n",
    "\n",
    "        try:\n",
    "            # Create the index\n",
    "            pc.create_index(\n",
    "                name=index_name,\n",
    "                dimension=embedding_dim,\n",
    "                metric='cosine',\n",
    "                spec=ServerlessSpec(\n",
    "                    cloud='aws', # Or 'aws', 'azure' - choose one available for current region/plan\n",
    "                    region='us-east-1' # Choose a region available for the current plan\n",
    "                )\n",
    "            )\n",
    "            print(f\"Index '{index_name}' created successfully. Please wait a moment for it to initialize...\")\n",
    "\n",
    "            # Optional: Wait loop checking index readiness\n",
    "            # import time\n",
    "            # while not pc.describe_index(index_name).status['ready']:\n",
    "            #     time.sleep(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating Pinecone index: {e}\")\n",
    "            pinecone_index = None # Ensure index object is None on error\n",
    "\n",
    "    else:\n",
    "        print(f\"Index '{index_name}' already exists. Connecting...\")\n",
    "\n",
    "    # Connect to the index\n",
    "    try:\n",
    "        pinecone_index = pc.Index(index_name)\n",
    "        print(f\"Connected to index '{index_name}'.\")\n",
    "        \n",
    "        # Describe index stats\n",
    "        print(pinecone_index.describe_index_stats())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Pinecone index '{index_name}': {e}\")\n",
    "        pinecone_index = None\n",
    "\n",
    "else:\n",
    "    print(\"Skipping index creation/connection as Pinecone client was not initialized.\")\n",
    "    pinecone_index = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c32adc",
   "metadata": {},
   "source": [
    "## Embed Chunks and Prepare for Upsert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59f8e3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing 50 chunks for embedding and upserting...\n",
      "Generating embeddings and preparing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing Chunks: 100%|██████████| 50/50 [00:00<00:00, 131979.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding and Upserting 50 vectors in batches of 100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upserting Batches: 100%|██████████| 1/1 [00:06<00:00,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished embedding and upserting all chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Check if chunks, model, and index connection are available\n",
    "if text_chunks and embedding_model and pinecone_index:\n",
    "    print(f\"Preparing {len(text_chunks)} chunks for embedding and upserting...\")\n",
    "\n",
    "    batch_size = 100 # Process chunks in batches for efficiency\n",
    "    vectors_to_upsert = [] # Temp list for batch upsert\n",
    "\n",
    "    # Prepare all data first\n",
    "    print(\"Generating embeddings and preparing data...\")\n",
    "    all_data_to_upsert = []\n",
    "    for i, chunk_text in enumerate(tqdm(text_chunks, desc=\"Preparing Chunks\")):\n",
    "         chunk_id = f\"chunk_{i}\" # Simple unique ID\n",
    "         \n",
    "         vector_data = {\n",
    "             \"id\": chunk_id,\n",
    "             \"metadata\": {\"text\": chunk_text} # Store original text as metadata\n",
    "         }\n",
    "\n",
    "         all_data_to_upsert.append(vector_data)\n",
    "\n",
    "    # Upsert data in batches\n",
    "    print(f\"Embedding and Upserting {len(all_data_to_upsert)} vectors in batches of {batch_size}...\")\n",
    "    for i in tqdm(range(0, len(all_data_to_upsert), batch_size), desc=\"Upserting Batches\"):\n",
    "        i_end = min(i + batch_size, len(all_data_to_upsert))\n",
    "        batch_data = all_data_to_upsert[i:i_end]\n",
    "\n",
    "        # Get texts for this batch\n",
    "        texts_to_embed = [item['metadata']['text'] for item in batch_data]\n",
    "\n",
    "        # Generate embeddings for the batch\n",
    "        try:\n",
    "            embeddings_batch = embedding_model.encode(texts_to_embed).tolist()\n",
    "\n",
    "            # Add embeddings to the batch data\n",
    "            vectors_for_pinecone = []\n",
    "            for j, item in enumerate(batch_data):\n",
    "                 vectors_for_pinecone.append({\n",
    "                     \"id\": item['id'],\n",
    "                     \"values\": embeddings_batch[j],\n",
    "                     \"metadata\": item['metadata']\n",
    "                 })\n",
    "\n",
    "            # Upsert the batch to Pinecone\n",
    "            pinecone_index.upsert(vectors=vectors_for_pinecone)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error embedding or upserting batch {i//batch_size + 1}: {e}\")\n",
    "            raise e # Re-raise the exception to stop execution if a batch fails\n",
    "\n",
    "    print(\"Finished embedding and upserting all chunks.\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping embedding/upserting due to missing chunks, model, or index connection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a9086f",
   "metadata": {},
   "source": [
    "## Perform a Test Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a78fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model and index are ready\n",
    "if embedding_model and pinecone_index:\n",
    "    print(\"\\nPerforming a test query...\")\n",
    "    query = \"What is the core idea of the attention mechanism?\"\n",
    "    print(f\"Test Query: '{query}'\")\n",
    "\n",
    "    try:\n",
    "        # 1. Embed the query\n",
    "        query_embedding = embedding_model.encode(query).tolist()\n",
    "\n",
    "        # 2. Query Pinecone\n",
    "        # Find the top N most similar chunks (e.g., top 3)\n",
    "        query_results = pinecone_index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=3, # Number of results to return\n",
    "            include_metadata=True # Get the original text back\n",
    "        )\n",
    "\n",
    "        # 3. Print results\n",
    "        print(\"\\nTop Search Results:\")\n",
    "        if query_results.matches:\n",
    "            for i, match in enumerate(query_results.matches):\n",
    "                print(f\"\\nResult {i+1}:\")\n",
    "                print(f\"  ID: {match.id}\")\n",
    "                print(f\"  Score (Similarity): {match.score:.4f}\")\n",
    "                \n",
    "                # Ensure metadata and text exist before printing\n",
    "                if match.metadata and 'text' in match.metadata:\n",
    "                    print(f\"  Text: {match.metadata['text'][:500]}...\") # Print start of the chunk\n",
    "                else:\n",
    "                    print(\"  Text: (Metadata or text missing)\")\n",
    "        else:\n",
    "            print(\"No matches found.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during the query: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping test query as model or index is not ready.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic_search_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
