{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a70c75d",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d0049ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai>=0.5.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (0.8.5)\n",
      "Requirement already satisfied: pinecone>=3.0.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (6.0.2)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (1.1.0)\n",
      "Requirement already satisfied: langchain in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (0.3.23)\n",
      "Requirement already satisfied: pypdf in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (5.4.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from google-generativeai>=0.5.0) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from google-generativeai>=0.5.0) (2.25.0rc1)\n",
      "Requirement already satisfied: google-api-python-client in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from google-generativeai>=0.5.0) (2.170.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from google-generativeai>=0.5.0) (2.40.2)\n",
      "Requirement already satisfied: protobuf in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from google-generativeai>=0.5.0) (5.29.4)\n",
      "Requirement already satisfied: pydantic in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from google-generativeai>=0.5.0) (2.11.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from google-generativeai>=0.5.0) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.0) (1.26.1)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from pinecone>=3.0.0) (2025.1.31)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from pinecone>=3.0.0) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from pinecone>=3.0.0) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from pinecone>=3.0.0) (2.3.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langchain) (0.3.51)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langchain) (0.3.27)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from google-api-core->google-generativeai>=0.5.0) (1.70.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai>=0.5.0) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai>=0.5.0) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai>=0.5.0) (4.9.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from pydantic->google-generativeai>=0.5.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from pydantic->google-generativeai>=0.5.0) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from pydantic->google-generativeai>=0.5.0) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from python-dateutil>=2.5.3->pinecone>=3.0.0) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from google-api-python-client->google-generativeai>=0.5.0) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from google-api-python-client->google-generativeai>=0.5.0) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from google-api-python-client->google-generativeai>=0.5.0) (4.1.1)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.0) (1.71.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.0) (1.71.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai>=0.5.0) (3.2.3)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai>=0.5.0) (0.6.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Required libraries checked/installed.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"google-generativeai>=0.5.0\" \"pinecone>=3.0.0\" tqdm python-dotenv langchain pypdf\n",
    "\n",
    "print(\"Required libraries checked/installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a281f015",
   "metadata": {},
   "source": [
    "## Load Environment Variables (Pinecone Credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b807951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading .env file...\n",
      "Pinecone API Key loaded.\n",
      "Google AI API Key loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Attempt to find and load the .env file to get environment variables\n",
    "dotenv_path = find_dotenv(raise_error_if_not_found=False) # Avoid error if .env is missing\n",
    "\n",
    "if dotenv_path:\n",
    "    print(\"Loading .env file...\")\n",
    "    load_dotenv(dotenv_path)\n",
    "    pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "    google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "    if not pinecone_api_key:\n",
    "        raise ValueError(\"Error: PINECONE_API_KEY not found in .env file.\")\n",
    "    else:\n",
    "        print(\"Pinecone API Key loaded.\")\n",
    "\n",
    "    if not google_api_key:\n",
    "        raise ValueError(\"Error: GOOGLE_API_KEY not found in .env file.\")\n",
    "    else:\n",
    "        print(\"Google AI API Key loaded.\")\n",
    "        \n",
    "else:\n",
    "    raise FileNotFoundError(\"Error: .env file not found. Please create one with your Pinecone and Google AI API keys.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0136b3",
   "metadata": {},
   "source": [
    "## Load and Chunk Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b7f0628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF from: ../data/Attention is All You Need.pdf\n",
      "Successfully loaded document with 39602 characters.\n",
      "Initializing text splitter...\n",
      "Splitting document into chunks...\n",
      "Document split into 50 chunks.\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "pdf_file_path = os.path.join('..', 'data', 'Attention is All You Need.pdf')\n",
    "text_chunks = [] # Initialize an empty list for chunks\n",
    "\n",
    "if not os.path.exists(pdf_file_path):\n",
    "    raise FileNotFoundError(f\"Error: PDF file not found at calculated path: {os.path.abspath(pdf_file_path)}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Loading PDF from: {pdf_file_path}\")\n",
    "    try:\n",
    "        # Initialize the PDF reader\n",
    "        reader = PdfReader(pdf_file_path)\n",
    "\n",
    "        # Extract text from the PDF\n",
    "        full_document_text = \"\"\n",
    "        for page_num, page in enumerate(reader.pages):\n",
    "            page_text = page.extract_text()\n",
    "\n",
    "            # Check if text was extracted from current page\n",
    "            if page_text:\n",
    "                full_document_text += page_text + \"\\n\"\n",
    "            else:\n",
    "                print(f\"Warning: No text extracted from page {page_num + 1}.\")\n",
    "\n",
    "        # Check if any text was extracted\n",
    "        if not full_document_text:\n",
    "             print(\"Warning: No text could be extracted from the PDF. Cannot proceed.\")\n",
    "        else:\n",
    "            print(f\"Successfully loaded document with {len(full_document_text)} characters.\")\n",
    "\n",
    "            # Initialize Recursive Character Text Splitter\n",
    "            print(\"Initializing text splitter...\")\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1000,\n",
    "                chunk_overlap=200,\n",
    "                length_function=len,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Priorize splitting by paragraph, then line, then space\n",
    "            )\n",
    "\n",
    "            # Split text into chunks\n",
    "            print(\"Splitting document into chunks...\")\n",
    "            text_chunks = text_splitter.split_text(full_document_text)\n",
    "            print(f\"Document split into {len(text_chunks)} chunks.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during PDF loading or chunking: {e}\")\n",
    "        raise e  # Re-raise the exception to stop execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda7267a",
   "metadata": {},
   "source": [
    "## Initialize Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adaed39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring Google AI Client...\n",
      "Google AI Client configured successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/semantic_search_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "google_ai_configured = False  # Flag to track successful configuration\n",
    "if google_api_key:\n",
    "    print(\"Configuring Google AI Client...\")\n",
    "    try:\n",
    "        genai.configure(api_key=google_api_key)\n",
    "        print(\"Google AI Client configured successfully.\")\n",
    "        google_ai_configured = True\n",
    "    except Exception as e:\n",
    "        print(f\"Error configuring Google AI Client: {e}\")\n",
    "else:\n",
    "    print(\"Skipping Google AI configuration due to missing API key.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65b78c5",
   "metadata": {},
   "source": [
    "## Initialize Pinecone Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cba2adf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Pinecone connection...\n",
      "Pinecone client initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = None \n",
    "if pinecone_api_key and google_ai_configured:\n",
    "    print(f\"Initializing Pinecone connection...\")\n",
    "    try:\n",
    "        pc = Pinecone(api_key=pinecone_api_key)\n",
    "        print(\"Pinecone client initialized successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Pinecone client: {e}\")\n",
    "else:\n",
    "    print(\"Skipping Pinecone initialization due to missing Pinecone API key or Google AI configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a71bb1",
   "metadata": {},
   "source": [
    "## Create or Connect to Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48fa6f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if Pinecone index 'semantic-search-app-index' exists...\n",
      "Required embedding dimension: 768\n",
      "Index 'semantic-search-app-index' already exists. Connecting...\n",
      "Connected to index 'semantic-search-app-index'.\n"
     ]
    }
   ],
   "source": [
    "pinecone_index = None\n",
    "if pc and google_ai_configured:  # Check both clients are ready\n",
    "    index_name = 'semantic-search-app-index'\n",
    "    embedding_dim = 768  # Dimension for Google's embedding-004 model\n",
    "\n",
    "    print(f\"Checking if Pinecone index '{index_name}' exists...\")\n",
    "    print(f\"Required embedding dimension: {embedding_dim}\")\n",
    "\n",
    "    existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "    if index_name not in existing_indexes:\n",
    "        print(f\"Index '{index_name}' does not exist. Creating...\")\n",
    "        try:\n",
    "            pc.create_index(\n",
    "                name=index_name,\n",
    "                dimension=embedding_dim,\n",
    "                metric='cosine',\n",
    "                spec=ServerlessSpec(\n",
    "                    cloud='aws',\n",
    "                    region='us-east-1'\n",
    "                )\n",
    "            )\n",
    "            print(f\"Index '{index_name}' created successfully. Please wait for initialization...\")\n",
    "\n",
    "            # Optional: Add wait loop if index isn't ready immediately\n",
    "            # import time\n",
    "            # while not pc.describe_index(index_name).status['ready']:\n",
    "            #     print(\"Waiting for index to be ready...\")\n",
    "            #     time.sleep(5)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating Pinecone index: {e}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"Index '{index_name}' already exists. Connecting...\")\n",
    "\n",
    "    try:\n",
    "        pinecone_index = pc.Index(index_name)\n",
    "        print(f\"Connected to index '{index_name}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Pinecone index '{index_name}': {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Skipping index creation/connection as Pinecone or Google AI client was not initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c32adc",
   "metadata": {},
   "source": [
    "## Embed Chunks and Prepare for Upsert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59f8e3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing 50 chunks for embedding and upserting via Google AI...\n",
      "Generating embeddings in batches of 100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Chunks: 100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Upserting 50 vectors to Pinecone in batches of 100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upserting Batches: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished embedding and upserting all chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "if text_chunks and google_ai_configured and pinecone_index:\n",
    "    print(f\"Preparing {len(text_chunks)} chunks for embedding and upserting via Google AI...\")\n",
    "\n",
    "    google_batch_size = 100  # Google's text embedding API limit for texts in a batch\n",
    "    pinecone_upsert_batch_size = 100  # Pinecone's recommended upsert batch size\n",
    "\n",
    "    model_name = 'models/text-embedding-004'  # Google's text embedding model\n",
    "\n",
    "    all_vectors_to_upsert = []\n",
    "\n",
    "    # Prepare all data with IDs and metadata first\n",
    "    for i, chunk_text in enumerate(text_chunks):\n",
    "         chunk_id = f\"chunk_{i}\"\n",
    "         all_vectors_to_upsert.append({\n",
    "             \"id\": chunk_id,\n",
    "             \"metadata\": {\"text\": chunk_text},\n",
    "             \"values\": []  # Placeholder for embedding values\n",
    "         })\n",
    "\n",
    "    # Embed in batches suitable for Google API\n",
    "    print(f\"Generating embeddings in batches of {google_batch_size}...\")\n",
    "    for i in tqdm(range(0, len(all_vectors_to_upsert), google_batch_size), desc=\"Embedding Chunks\"):\n",
    "        i_end = min(i + google_batch_size, len(all_vectors_to_upsert))\n",
    "        current_batch_items = all_vectors_to_upsert[i:i_end]\n",
    "        texts_in_current_batch = [item['metadata']['text'] for item in current_batch_items]\n",
    "\n",
    "        try:\n",
    "            response = genai.embed_content(model=model_name,\n",
    "                                           content=texts_in_current_batch,\n",
    "                                           task_type=\"retrieval_document\")\n",
    "            \n",
    "            embeddings_from_api = response['embedding']  # This is a list of lists\n",
    "\n",
    "            # Assign embeddings back to the items in all_vectors_to_upsert\n",
    "            for j, embedding_vector in enumerate(embeddings_from_api):\n",
    "                all_vectors_to_upsert[i+j]['values'] = embedding_vector\n",
    "            \n",
    "            # Optional: If hitting rate limits, add a small delay\n",
    "            # import time\n",
    "            # time.sleep(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error embedding batch starting at index {i}: {e}\")\n",
    "            raise e  # Stop execution if embedding fails\n",
    "\n",
    "    # Upsert all prepared vectors to Pinecone in batches\n",
    "    if all_vectors_to_upsert and all_vectors_to_upsert[0]['values']:  # Check if embeddings were generated\n",
    "        print(f\"\\nUpserting {len(all_vectors_to_upsert)} vectors to Pinecone in batches of {pinecone_upsert_batch_size}...\")\n",
    "        for i in tqdm(range(0, len(all_vectors_to_upsert), pinecone_upsert_batch_size), desc=\"Upserting Batches\"):\n",
    "            i_end = min(i + pinecone_upsert_batch_size, len(all_vectors_to_upsert))\n",
    "            pinecone_batch_to_upsert = all_vectors_to_upsert[i:i_end]\n",
    "\n",
    "            try:\n",
    "                pinecone_index.upsert(vectors=pinecone_batch_to_upsert)\n",
    "            except Exception as e:\n",
    "                print(f\"Error upserting batch to Pinecone starting at index {i}: {e}\")\n",
    "                raise e  # Stop execution if upsert fails\n",
    "            \n",
    "        print(\"Finished embedding and upserting all chunks.\")\n",
    "        # Optional: Check index stats\n",
    "        # print(pinecone_index.describe_index_stats())\n",
    "\n",
    "    else:\n",
    "        print(\"No vectors with embeddings were prepared for upserting.\")\n",
    "        \n",
    "else:\n",
    "    print(\"Skipping embedding/upserting due to missing dependencies (text_chunks, Google AI config, or Pinecone index).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a9086f",
   "metadata": {},
   "source": [
    "## Perform a Test Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6a78fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing a test query...\n",
      "Test Query: 'What is the core idea of attention mechanism?'\n",
      "\n",
      "Top Search Results:\n",
      "\n",
      "Result 1:\n",
      "  ID: chunk_12\n",
      "  Score (Similarity): 0.6808\n",
      "  Text: into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\n",
      "the matrix of outputs as:\n",
      "Attention(Q, K, V) = softmax(QKT\n",
      "√dk\n",
      ")V (1)\n",
      "The two most commonly used attention functions are additive attention [2], and dot-product (multi-\n",
      "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
      "of 1√dk\n",
      ". Additive attention computes the compatibility function using a feed-forward network with\n",
      "a single hidden layer. While t...\n",
      "\n",
      "Result 2:\n",
      "  ID: chunk_11\n",
      "  Score (Similarity): 0.6770\n",
      "  Text: where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "3\n",
      "Scaled Dot-Product Attention\n",
      " Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "query with the corresponding key.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled D...\n",
      "\n",
      "Result 3:\n",
      "  ID: chunk_6\n",
      "  Score (Similarity): 0.6713\n",
      "  Text: The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\n",
      "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This ...\n"
     ]
    }
   ],
   "source": [
    "if google_ai_configured and pinecone_index:\n",
    "    print(\"\\nPerforming a test query...\")\n",
    "    query = \"What is the core idea of attention mechanism?\"\n",
    "    print(f\"Test Query: '{query}'\")\n",
    "    model_name_for_query = 'models/text-embedding-004'\n",
    "\n",
    "    try:\n",
    "        # 1. Embed the query using Google AI API\n",
    "        response = genai.embed_content(model=model_name_for_query,\n",
    "                                       content=query,\n",
    "                                       task_type=\"retrieval_query\")\n",
    "        query_embedding = response['embedding']\n",
    "\n",
    "        # 2. Query Pinecone\n",
    "        query_results = pinecone_index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=3,\n",
    "            include_metadata=True\n",
    "        )\n",
    "\n",
    "        # 3. Print results\n",
    "        print(\"\\nTop Search Results:\")\n",
    "        if query_results.matches:\n",
    "            for i, match in enumerate(query_results.matches):\n",
    "                print(f\"\\nResult {i+1}:\")\n",
    "                print(f\"  ID: {match.id}\")\n",
    "                print(f\"  Score (Similarity): {match.score:.4f}\")\n",
    "                if match.metadata and 'text' in match.metadata:\n",
    "                    print(f\"  Text: {match.metadata['text'][:500]}...\")\n",
    "                else:\n",
    "                    print(\"  Text: (Metadata or text missing)\")\n",
    "        else:\n",
    "            print(\"No matches found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during the query: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Skipping test query as Google AI config or Pinecone index is not ready.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic_search_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
